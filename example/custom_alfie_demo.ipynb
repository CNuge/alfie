{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The alfie package - demo\n",
    "\n",
    "\n",
    "\n",
    "## Part 1. Evaluate sequences with alfie's pre-built kingdom level classifier\n",
    "\n",
    "Alfie's primary function is as a kingdom-level taxonomic classifier for COI-5P barcode data. To accomplish this, alfie uses a deep neural network to analyze a set of input sequences and predict taxonomy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data \n",
    "\n",
    "Alfie contains a series of functions for reading and writing DNA sequence data in fasta or fastq format. These functions are found in the `seqio` module and their import is demonstrated below. Alfie also contains two example files that we import below using the read_fasta and read_fastq functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions for fasta/fastq input and output \n",
    "from alfie.seqio import read_fasta, read_fastq, write_fasta, write_fastq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': '@seq1_plantae',\n",
       " 'sequence': 'ttctaggagcatgtatatctatgctaatccgaatggaattagctcaaccaggtaaccatttgcttttaggtaatcaccaagtatacaatgttttaattacagcacatgcttttttaatgattttttttatggtaatgcctgtaatgattggtggttttggtaattggttagttcctattatgataggaagtccagatatggcttttcctagactaaataacatatctttttgacttcttccaccttctttatgtttacttttagcttcttcaatggttgaagtaggtgttggaacaggatgaactgtttatcctccccttagttcgatacaaagtcattcaggcggagctgttgatttagcaatttttagcttacatttatctggagcttcatcgattttaggagctgtcaattttatttctacgattctaaatatgcgtaatcctgggcaaagcatgtatcgaatgccattatttgtttgatctatttttgtaacggca',\n",
       " 'strand': '+',\n",
       " 'quality': '~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import path to example file\n",
    "from alfie import ex_fastq_file\n",
    "\n",
    "#read in example file\n",
    "example_fastq = read_fastq(ex_fastq_file)\n",
    "\n",
    "#check format\n",
    "example_fastq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'seq1_plantae',\n",
       " 'sequence': 'TTCTAGGAGCATGTATATCTATGCTAATCCGAATGGAATTAGCTCAACCAGGTAACCATTTGCTTTTAGGTAATCACCAAGTATACAATGTTTTAATTACAGCACATGCTTTTTTAATGATTTTTTTTATGGTAATGCCTGTAATGATTGGTGGTTTTGGTAATTGGTTAGTTCCTATTATGATAGGAAGTCCAGATATGGCTTTTCCTAGACTAAATAACATATCTTTTTGACTTCTTCCACCTTCTTTATGTTTACTTTTAGCTTCTTCAATGGTTGAAGTAGGTGTTGGAACAGGATGAACTGTTTATCCTCCCCTTAGTTCGATACAAAGTCATTCAGGCGGAGCTGTTGATTTAGCAATTTTTAGCTTACATTTATCTGGAGCTTCATCGATTTTAGGAGCTGTCAATTTTATTTCTACGATTCTAAATATGCGTAATCCTGGGCAAAGCATGTATCGAATGCCATTATTTGTTTGATCTATTTTTGTAACGGCA'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#repeat the above process with a fasta file\n",
    "from alfie import ex_fasta_file\n",
    "\n",
    "example_fasta = read_fasta(ex_fasta_file)\n",
    "\n",
    "example_fasta[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify sequences\n",
    "\n",
    "Once we have imported the data with the seqio module, we can use the `classify_records` function to obtain taxonomic classifications for the input sequences.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import classification function\n",
    "from alfie.classify import classify_records\n",
    "\n",
    "seq_records, predictions = classify_records(example_fasta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification process is completed above in just one line of code. In the background the `classify_records` function is taking the input sequences, generating a set of input features (k-mer frequencies) for each sequence, and passing the feature sets through a neural network to obtain a prediction. \n",
    "\n",
    "The function yields two outputs, a list of sequence records and an array classifications. The sequence records are a list of dictionaries, like the inputs, but now with an additional 'kmer_data' entry which contains a class instance used to generate the input features for the neural network (more information on the `KmerFeatures` class is provided in the following section on training a classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'seq1_plantae',\n",
       " 'sequence': 'TTCTAGGAGCATGTATATCTATGCTAATCCGAATGGAATTAGCTCAACCAGGTAACCATTTGCTTTTAGGTAATCACCAAGTATACAATGTTTTAATTACAGCACATGCTTTTTTAATGATTTTTTTTATGGTAATGCCTGTAATGATTGGTGGTTTTGGTAATTGGTTAGTTCCTATTATGATAGGAAGTCCAGATATGGCTTTTCCTAGACTAAATAACATATCTTTTTGACTTCTTCCACCTTCTTTATGTTTACTTTTAGCTTCTTCAATGGTTGAAGTAGGTGTTGGAACAGGATGAACTGTTTATCCTCCCCTTAGTTCGATACAAAGTCATTCAGGCGGAGCTGTTGATTTAGCAATTTTTAGCTTACATTTATCTGGAGCTTCATCGATTTTAGGAGCTGTCAATTTTATTTCTACGATTCTAAATATGCGTAATCCTGGGCAAAGCATGTATCGAATGCCATTATTTGTTTGATCTATTTTTGTAACGGCA',\n",
       " 'kmer_data': <alfie.kmerseq.KmerFeatures at 0x7f96b925d210>}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_records[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions returned are an encoded array of kingdom classifications.\n",
    "Encodings are in alphabetical order: \n",
    "```\n",
    "0 == \"animalia\", 1 == \"bacteria\", 2 == \"fungi\", 3 == \"plantae\", 4 == \"protista\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1, 4, 0, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some purposes, the names corresponding to the encoded predictions may be preferable to the numeric encodings. To obtain these, use the `decode_predictions` to move from the array of numeric predictions to a list of names.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plantae', 'bacteria', 'protista', 'animalia', 'animalia']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from alfie.classify import decode_predictions\n",
    "\n",
    "predicted_kingdoms = decode_predictions(predictions)\n",
    "predicted_kingdoms[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working within python provides the freedom to manipulate the sequence records in various ways using this information. What you do with the classification information will depend on your research goal, you may wish to save a select category of sequences to a file, merge the classifications with the existing sequence ids, or carry the classifications through to additional analyses in python. Here we explore a few of these possibilities.\n",
    "\n",
    "\n",
    "**a.** Add the predictions into the sequence dictionaries prior to subsequent manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'seq1_plantae',\n",
       " 'sequence': 'TTCTAGGAGCATGTATATCTATGCTAATCCGAATGGAATTAGCTCAACCAGGTAACCATTTGCTTTTAGGTAATCACCAAGTATACAATGTTTTAATTACAGCACATGCTTTTTTAATGATTTTTTTTATGGTAATGCCTGTAATGATTGGTGGTTTTGGTAATTGGTTAGTTCCTATTATGATAGGAAGTCCAGATATGGCTTTTCCTAGACTAAATAACATATCTTTTTGACTTCTTCCACCTTCTTTATGTTTACTTTTAGCTTCTTCAATGGTTGAAGTAGGTGTTGGAACAGGATGAACTGTTTATCCTCCCCTTAGTTCGATACAAAGTCATTCAGGCGGAGCTGTTGATTTAGCAATTTTTAGCTTACATTTATCTGGAGCTTCATCGATTTTAGGAGCTGTCAATTTTATTTCTACGATTCTAAATATGCGTAATCCTGGGCAAAGCATGTATCGAATGCCATTATTTGTTTGATCTATTTTTGTAACGGCA',\n",
       " 'kmer_data': <alfie.kmerseq.KmerFeatures at 0x7f96b925d210>,\n",
       " 'kingdom': 'plantae'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#iterate over the predictions, use enumerate to get record number\n",
    "for i, p in enumerate(predicted_kingdoms):\n",
    "    #call corresponding sequence record and add a kingdom entry to dictionary\n",
    "    seq_records[i]['kingdom'] = p\n",
    "    \n",
    "#taxonomic classification now present in the sequence dict\n",
    "seq_records[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** Use the predictions to subset out only data from a kingdom of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_sequences = []\n",
    "\n",
    "for i, x in enumerate(predicted_kingdoms):\n",
    "    if x == 'animalia':\n",
    "        animal_sequences.append(seq_records[i])\n",
    "        \n",
    "#Note: you could avoid the transition to string classifications and \n",
    "#subset using the numeric classifications in the `predictions` array \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we see that the resulting list `animal_sequences` contains only the kingdom 'animalia'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'seq4_animalia',\n",
       "  'sequence': 'AATCCGGGATCATTAATTGGTGATGATCAAATTTATAATACCATTGTTACAGCTCATGCATTTATTATAATTTTTTTTATGGTTATACCAATTATAATCGGAGGATTTGGTAATTGATTAGTACCATTGATATTAGGGGCACCTGATATAGCTTTCCCACGAATAAATAATATAAGATTTTGATTACTACCCCCTTCTTTAATACTTCTAATTTCTAGTAGTATTGTAGAAAATGGAGCTGGAACTGGATGAACAGTTTACCCCCCTTTATCATCTAATATCGCCCATGGAGGAAGATCTGTTGACTTAGCTATTTTTTCATTACATTTAGCTGGTATTTCATCTATTTTAGGAGCTATTAATTTTATT',\n",
       "  'kmer_data': <alfie.kmerseq.KmerFeatures at 0x7f96b912cfd0>,\n",
       "  'kingdom': 'animalia'},\n",
       " {'name': 'seq5_animalia',\n",
       "  'sequence': 'CAAATTTATAATACAATTGTTACAGCCCATGCTTTTATTATAATTTTCTTTATAGTAATGCCTATTATAATTGGAGGATTTGGAAATTGATTAGTACCTTTAATATTAGGAGCCCCCGATATAGCTTTCCCCCGAATAAATAATATAAGATTTTGACTTCTCCCCCCATCATTAACCCTTTTAATTTCAAGAAGAGTTGTAGAAAATGGTACTGGAACTGGATGAACAGTTTACCCCCCTTTATCATCTAATATTGCTCATAGAGGAAGATCTGTTGATTTATCTATTTTTTCCCTTCATTTAGCTGGAATTTCTTCTATTTTAGGAGCAATTAATTTTATTACAACTATTATTAATATACGATTAAATAATATAACATTTGATCAATTACCTTTATTTGTATGATCTGTTGGAATTACAGCTCTTCTTCTTCTTCTTTCTCTTCCTGTTTTAGCAGGAGCTATTACTATATTATTA',\n",
       "  'kmer_data': <alfie.kmerseq.KmerFeatures at 0x7f96b9131150>,\n",
       "  'kingdom': 'animalia'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animal_sequences[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.** Writing to file\n",
    "\n",
    "Once processing and analyses are completed, we may wish to save our sequence records to a new output file. Sequences, or lists of sequences in dictionary format can be written to output files if they possess the proper set of keys ('name' and 'sequence' for fasta, 'name', 'sequence', 'strand', and 'quality'). Additional keys will be ignored when writing the output.\n",
    "\n",
    "In this demonstration, we take the animal sequences we isolated from the input (**b.**) and write them to a new fasta file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you uncomment and run the following line, it will make an output file in your current working directory\n",
    "#write_fasta( animal_sequences, 'animalia_example_output.fasta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is all there is to deploying the alfie package as a kingdom level classifier from within Python. The kingdom level classification provides an efficient means of separating DNA sequences from a target kingdom from the large amount of off-target noise that can exist within a metabarcoding or environmental DNA data set.\n",
    "\n",
    "Next, we explore how the functionality of alfie can be customized to allow for isolation of target sequences on finer taxonomic scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2. Train and test a custom, alignment-free taxonomic classifier\n",
    "\n",
    "In addition to using alfie as a kingdom level classifier, alfie's helper functions can be used to train a custom DNA barcode classification model. Custom model construction will allow for the general functionality of alfie (as a kingdom-level classifier) to be extended and specialized. Some common applications of this customization may be the training of a classifier for a sub-group of interest (i.e. an intra-group classifier, which is demonstrated here for the phylum annelida), or training a binary classifier to isolate barcodes from a specific taxonomic group (i.e. a classifier that says whether an input sequence is or is not a sequence from a teleost fish).\n",
    "\n",
    "The following demonstration will show how to train a custom binary or multiclass neural network through a combination of alfie, scikit learn, and tensorflow. Note this process is a little more involved than the default implementation of alfie. This demo assumes the reader has an understanding of the basics of data science and machine learning in Python. If you're not yet comfortable with those topics, I would recommend the book [Hands-on Machine Learning with Scikit-Learn and TensorFlow](https://github.com/ageron/handson-ml2) as a good starting point.\n",
    "\n",
    "It is also important to note that your mileage with a design and deployment of a custom classifier will vary with the quality of the training data you use. A few thousand labelled sequences at a minimum is recommended. If you're looking to acquire COI training data, consider: [the BOLD data systems website](http://www.boldsystems.org/index.php/Login/page?destination=MAS_Management_UserConsole), or [subsetting the data used in training the original alfie neural network](https://github.com/CNuge/data-alfie). \n",
    "\n",
    "If you want training data for other barcodes or genes have a look at [NCBI](https://www.ncbi.nlm.nih.gov) (warning: data mining and cleaning required), or other online barcode data sources such as the [PLANiTS dataset](https://github.com/apallavicini/PLANiTS). Another good source of barcode data is [Dr. Teresita Porter's GitHub page](https://github.com/terrimporter), which contains trained RDP Classifiers (and labelled training data!) for rbcL, 18S, ITS, and other barcodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alfie.kmerseq import KmerFeatures\n",
    "from alfie.training import stratified_taxon_split, sample_seq, process_sequences, alfie_dnn_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the demo data\n",
    "\n",
    "The demo data can be found in the [alfie GitHub repository](https://github.com/CNuge/alfie/tree/master/example). The relative import below assumes you have downloaded the alfie repository from Github and that you are working directory is: `alfie/example`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('alfie_small_train_example.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, this demo is conducted with 10,000 sequences from the phylum Annelida, which has only two taxonomic classes. We will train a neural network to predict the class of Annelida sequences in an alignment-free fashion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processid</th>\n",
       "      <th>sequence</th>\n",
       "      <th>phylum</th>\n",
       "      <th>class</th>\n",
       "      <th>order</th>\n",
       "      <th>family</th>\n",
       "      <th>genus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>GAHAP309-13</td>\n",
       "      <td>accttatactttattctgggcgtatgagcaggaatattgggtgcag...</td>\n",
       "      <td>Annelida</td>\n",
       "      <td>Clitellata</td>\n",
       "      <td>Enchytraeida</td>\n",
       "      <td>Enchytraeidae</td>\n",
       "      <td>Grania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>GAHAP2002-14</td>\n",
       "      <td>accctatatttcattctcggagtttgagctggcatagtaggtgccg...</td>\n",
       "      <td>Annelida</td>\n",
       "      <td>Clitellata</td>\n",
       "      <td>Haplotaxida</td>\n",
       "      <td>Lumbricidae</td>\n",
       "      <td>Aporrectodea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>GBAN15302-19</td>\n",
       "      <td>actctatacttaatttttggtatt-gagccggtatagtaggaacag...</td>\n",
       "      <td>Annelida</td>\n",
       "      <td>Clitellata</td>\n",
       "      <td>Haplotaxida</td>\n",
       "      <td>Naididae</td>\n",
       "      <td>Ainudrilus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>GBAN11905-19</td>\n",
       "      <td>acactatattttattttaggaatttgagctggaataattggagcag...</td>\n",
       "      <td>Annelida</td>\n",
       "      <td>Clitellata</td>\n",
       "      <td>Crassiclitellata</td>\n",
       "      <td>Megascolecidae</td>\n",
       "      <td>Metaphire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>GBAN15299-19</td>\n",
       "      <td>acattatacctaattta-ggtgtatgagccggaatagttggaacag...</td>\n",
       "      <td>Annelida</td>\n",
       "      <td>Clitellata</td>\n",
       "      <td>Haplotaxida</td>\n",
       "      <td>Naididae</td>\n",
       "      <td>Ainudrilus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      processid                                           sequence    phylum  \\\n",
       "0   GAHAP309-13  accttatactttattctgggcgtatgagcaggaatattgggtgcag...  Annelida   \n",
       "1  GAHAP2002-14  accctatatttcattctcggagtttgagctggcatagtaggtgccg...  Annelida   \n",
       "2  GBAN15302-19  actctatacttaatttttggtatt-gagccggtatagtaggaacag...  Annelida   \n",
       "3  GBAN11905-19  acactatattttattttaggaatttgagctggaataattggagcag...  Annelida   \n",
       "4  GBAN15299-19  acattatacctaattta-ggtgtatgagccggaatagttggaacag...  Annelida   \n",
       "\n",
       "        class             order          family         genus  \n",
       "0  Clitellata      Enchytraeida   Enchytraeidae        Grania  \n",
       "1  Clitellata       Haplotaxida     Lumbricidae  Aporrectodea  \n",
       "2  Clitellata       Haplotaxida        Naididae    Ainudrilus  \n",
       "3  Clitellata  Crassiclitellata  Megascolecidae     Metaphire  \n",
       "4  Clitellata       Haplotaxida        Naididae    Ainudrilus  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Clitellata    6187\n",
       "Polychaeta    3813\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conducting a train/test split\n",
    "\n",
    "First we sequester a test set from the input data. The aflie function `stratified_taxon_split` can be used to split a dataframe in a stratified fashion based on the taxonomic data in a column. This ensures that each taxonomic group is evenly represented in the training and test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conducting train/test split, split evenly by: class\n"
     ]
    }
   ],
   "source": [
    "train, test = stratified_taxon_split(data, class_col = 'class', test_size = 0.3, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call some summary functions on the output dataframes to verify the even split of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (7000, 7)\n",
      "Clitellata    4331\n",
      "Polychaeta    2669\n",
      "Name: class, dtype: int64\n",
      "\n",
      "\n",
      "test data shape: (3000, 7)\n",
      "Clitellata    1856\n",
      "Polychaeta    1144\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape:\",train.shape)\n",
    "print(train['class'].value_counts())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"test data shape:\", test.shape)\n",
    "print(test['class'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the predictor data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kmer features\n",
    "\n",
    "Alfie contains a custom python class called KmerFeatures, which intakes a DNA sequence and generates k-mer count and k-mer frequency data. This class is used in the background by the `classify_records` function to generate the features for neural network prediction. \n",
    "\n",
    "Here we will use it to take our data and generate the feature sets for model training. The class takes an id and a DNA sequence, by default it will count 4mer frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment to look at the docs\n",
    "#?KmerFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<alfie.kmerseq.KmerFeatures at 0x7f963c2253d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = KmerFeatures(name = train['processid'][0]  , sequence = train['sequence'][0])\n",
    "x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon initiation, the KmerFeatures class instance will generate a k-mer count dictionary, where the keys are all the nucleotide permutations for size 'k'. The class then iterates through the input sequence and counts the occurrences of each k-mer. Any occurrences of nucleotides other than A, T, G, and C will cause the encompassing k-mer to be ignored.\n",
    "\n",
    "After initiation, the k-mer keys and values can be accessed like a regular python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AAAA',\n",
       " 'AAAC',\n",
       " 'AAAG',\n",
       " 'AAAT',\n",
       " 'AACA',\n",
       " 'AACC',\n",
       " 'AACG',\n",
       " 'AACT',\n",
       " 'AAGA',\n",
       " 'AAGC']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.keys()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Note/digression: later on, we rely on the fact keys are from an alphabetically ordered dict (the default for python 3.6 or newer). Alfie sorts the dict items by key before returning them just to be safe (Python 3.6 and later are ordered dicts by default so this is redundant). This is a PSA that it is 2020 and time to update if you're on 3.5 or earlier!](https://twitter.com/raymondh/status/773978885092323328)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 1, 2, 2, 3, 1, 4, 3, 1]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.values()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AAAA', 2),\n",
       " ('AAAC', 3),\n",
       " ('AAAG', 1),\n",
       " ('AAAT', 2),\n",
       " ('AACA', 2),\n",
       " ('AACC', 3),\n",
       " ('AACG', 1),\n",
       " ('AACT', 4),\n",
       " ('AAGA', 3),\n",
       " ('AAGC', 1)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.items()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that the training data is not biased by the overall size of the sequences, it is best to train the models using the k-mer frequencies (count/total), which the KmerFeatures class also provides for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0030722 , 0.00460829, 0.0015361 , 0.0030722 , 0.0030722 ,\n",
       "       0.00460829, 0.0015361 , 0.00614439, 0.00460829, 0.0015361 ])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.freq_values()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For machine learning purposes, the KmerFeatures class also outputs the k-mer dictionary keys and value frequencies as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AAAA', 'AAAC', 'AAAG', 'AAAT', 'AACA'], dtype='<U4')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.labels[:5] #only showing first 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0030722 , 0.00460829, 0.0015361 , 0.0030722 , 0.0030722 ])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.kmer_freqs[:5] #just static method version of x1.freq_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By passing the optional `k` argument, we can overrule the default k-mer size of 4 and specify a custom k-mer dictionary size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of 5mer counts:\n",
      "[('AAAAA', 1), ('AAAAC', 0), ('AAAAG', 1), ('AAAAT', 0), ('AAACA', 0)]\n",
      "1mer counts:\n",
      "[('A', 184), ('C', 134), ('G', 110), ('T', 226)]\n"
     ]
    }
   ],
   "source": [
    "# count 5mers \n",
    "x5mer = KmerFeatures(name = train['processid'][0]  , sequence = train['sequence'][0], k = 5)\n",
    "\n",
    "print(\"start of 5mer counts:\")\n",
    "print(x5mer.items()[:5])\n",
    "\n",
    "#generate 5mer and 1mer frequencies\n",
    "x1_mer = KmerFeatures(name = train['processid'][0]  , sequence = train['sequence'][0], k = 1)\n",
    "print(\"1mer counts:\")\n",
    "print(x1_mer.items())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random subsampling\n",
    "\n",
    "The k-mer encoding of the predictor data above utilizes whole barcode records. Often in analysis of metabarcode or eDNA research we are not dealing with complete barcode sequences, but rather short sequences (from primer-defined barcode sub-sections in the case of metabarcoding data, or undefined sub-sections in the case of metagenomics data). \n",
    "\n",
    "To train our classification model on data that more closely resembles real world sequences, we can use the alfie's `sample_seq` function to randomly subsample the training sequences. We specify the `min_size` and `max_size` of the subsequence and the function will randomly generate a subsample of the sequence in the defined size range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?sample_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ttctcccattaatacttggagcccctgacatagcattcccacgattaaataatataagattttgattactacctccgtctctcattcttcttgtttcatccgcagctgttgaaaaaggagcaggaacaggttgaactgtatatccacctctagcaaggaatttagcacatgcaggtccttcagtagatttagcaatcttttcacttcatctcgcaggtgcttcctctattttaggtgcagtaaactttattactacagtaattaatatgcgttgacaaggaatctctctagaacgaatccccttatttgtatgagctgtagctattacagttgttctattacttttatctcttccagttcttgctggagccattactatattattaaccgatcgaaacctaaatacttcattttttga']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#demonstrate on a single sequence - i.e. row 1 of the train\n",
    "\n",
    "sub_seq = sample_seq(train['sequence'][0])\n",
    "sub_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the sample seq function upsampling of data as well. If we enter an integer value for the  `n` argument to the function then `n` random subsamples will be generated. The function therefore returns a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accaggatcatttttaggaagagaccaactatataatacaattgtcacagcacatgcatttttaataattttttttctagttataccagtatttattggcggatttggaaactgacttctcccattaatacttggagcccctgacatagcattcccacgattaaataatataagattttgattactacctccgtctctcattcttcttgtttcatccgcagctgttgaaaaaggagcaggaacaggttgaactgtatatccacctctagcaaggaatttagcacatgcaggtccttcagtagatttagcaatcttttcacttcatctcgcaggtgcttcctctattttaggtgcagtaaactttattactacagtaattaatatgcgttgacaaggaatctctctagaacgaatccccttatttgtatgagctgtagctattacagttgttctattacttttatctcttccagttcttgctggagccattactatattattaaccgatcgaaacctaaatacttcatttttt',\n",
       " 'ttatactttattctgggcgtatgagcaggaatattgggtgcagcgataagattgttaattcgaattgaattaagccaaccaggatcatttttaggaagagaccaactatataatacaattgtcacagcacatgcatttttaataattttttttctagttataccagtatttattggcggatttggaaactgacttctcccattaatacttggagcccctgacatagcattcccacgattaaataatataagattttgattactacctccgtctctcattcttcttgtttcatccgcagctgttgaaaaaggagcaggaacaggttgaactgtatatccacctctagcaaggaatttagcacatgcaggtccttcagtagatttagcaatcttttcacttcatctcgcaggtgcttcctctattttaggtgcagtaaactttattactacagtaattaatatgcgttgacaaggaatctctctagaacgaatccccttatttgtatgagctgtagctattacagttgttct']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_seq = sample_seq(train['sequence'][0], n = 10)\n",
    "sub_seq[:2] #different random subsections of the same input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch processing and model input generation\n",
    "\n",
    "We can batch process sequences, taking random subsamples, generating `KmerFeatures` class instances, and extracting the k-mer frequencies from the objects using the `process_sequences` function. This function takes in a dataframe that contains the ids, sequences and labels in different columns. The function will generate a dictionary that contains four lists of: ids, sequences, k-mer frequency arrays, and labels for each sequence.\n",
    "\n",
    "Here both the train and test data are processed with the default k-mer size (`k = 4`) and a single subsample per sequence in the input dataframe (`n = 1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?process_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kmer_data = process_sequences(train, label_col = 'class')\n",
    "test_kmer_data = process_sequences(test, label_col = 'class')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ids', 'labels', 'data', 'seq'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_kmer_data.keys() #four keys, each with lists of equal size containing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output dictionaries of the process_sequences function can be easily turned into numpy arrays, which are compatible inputs for most machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building X arrays\n"
     ]
    }
   ],
   "source": [
    "print(\"building X arrays\")\n",
    "X_train = np.array(train_kmer_data['data'])\n",
    "X_test = np.array(test_kmer_data['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the response data\n",
    "In addition to the alignment-fre predictor data, we also need to generate our response data. We can use scikit learn's `LabelBinarizer` function to numerically encode the taxonomic information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding y arrays\n",
      "names are encoded in alphabetical order:\n",
      "['Clitellata' 'Polychaeta']\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "(7000, 1)\n",
      "(3000, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"encoding y arrays\")\n",
    "#encode the y labels\n",
    "y_train_raw =  train_kmer_data['labels']\n",
    "y_test_raw = test_kmer_data['labels']\n",
    "\n",
    "tax_encoder = LabelBinarizer()\n",
    "\n",
    "y_train = tax_encoder.fit_transform(y_train_raw)\n",
    "y_test = tax_encoder.transform(y_test_raw)\n",
    "\n",
    "print(\"names are encoded in alphabetical order:\")\n",
    "print(tax_encoder.classes_)\n",
    "\n",
    "# note since this is a binary class the y arrays contain only 0 and 1\n",
    "# if multiclass, we need to take the argmax for multilabels \n",
    "#y_train = np.argmax(y_train, axis = 1)\n",
    "#y_test = np.argmax(y_train, axis = 1)\n",
    "\n",
    "print(y_train[:10])\n",
    "print(y_train[-10:])\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uncomment these lines to save all the generated arrays \n",
    "#print(\"saving X to files\")\n",
    "#np.save('X_train.npy', X_train)\n",
    "#np.save('X_test.npy', X_test)\n",
    "\n",
    "#print(\"saving y to files\")\n",
    "#np.save('y_train.npy', y_train)\n",
    "#np.save('y_test.npy', y_test)\n",
    "\n",
    "##can then load with np.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the neural network\n",
    "\n",
    "After the train and test data are generated, we can use the `alfie_dnn_default` function to train a generic tensorflow neural network. We just need to specify the size of the hidden neuron layers, the shape of the input, and the number of classes we are predicting. The default dropout rate of 0.2 can be changed but will suit most situations. If you want to exert more control over the network architecture, then feel free to build your own and skip down to the section `Deploying custom trained models` for integrating it with alfie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?alfie_dnn_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "annelida_params = {\n",
    "    'hidden_sizes' : [16,64,128,64,16], # the number of neurons in our hidden layers \n",
    "    'dropout' : 0.2, #the dropout rate for the neural networl\n",
    "    'in_shape' : 256, #default for 4mers, can determine this through the shape of the X dataframe (# columns)  \n",
    "    'n_classes' : 2, #we have two annelida classes. Default is 5 (number of kingdoms).    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "annelida_classifier = alfie_dnn_default(**annelida_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 100)               25700     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                1616      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                1088      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 46,054\n",
      "Trainable params: 46,054\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#look at the neural network architecture\n",
    "annelida_classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the classifier is trained in an extremely simple fashion, the data is passed through the model in batches of 100 for 50 epochs. We could improve this model with cross validation and hyperparameter tuning (i.e. changing the dropout or the number of hidden layers). Additionally you could change the set of input features (size of k), or take lower level control and design a neural network yourself. These refinements and optimizations are outside of the scope of the alfie tutorial. Scikit learn and tensorflow have excellent functions and documentation if you wish to branch out beyond the `alfie_dnn_default` architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples\n",
      "Epoch 1/50\n",
      "7000/7000 [==============================] - 2s 339us/sample - loss: 0.6113 - accuracy: 0.6239\n",
      "Epoch 2/50\n",
      "7000/7000 [==============================] - 1s 163us/sample - loss: 0.3232 - accuracy: 0.9110\n",
      "Epoch 3/50\n",
      "7000/7000 [==============================] - 1s 170us/sample - loss: 0.1583 - accuracy: 0.9476\n",
      "Epoch 4/50\n",
      "7000/7000 [==============================] - 1s 173us/sample - loss: 0.1298 - accuracy: 0.9594\n",
      "Epoch 5/50\n",
      "7000/7000 [==============================] - 1s 149us/sample - loss: 0.1185 - accuracy: 0.9629\n",
      "Epoch 6/50\n",
      "7000/7000 [==============================] - 1s 160us/sample - loss: 0.0990 - accuracy: 0.9696\n",
      "Epoch 7/50\n",
      "7000/7000 [==============================] - 1s 155us/sample - loss: 0.1249 - accuracy: 0.9616\n",
      "Epoch 8/50\n",
      "7000/7000 [==============================] - 1s 178us/sample - loss: 0.0924 - accuracy: 0.9711\n",
      "Epoch 9/50\n",
      "7000/7000 [==============================] - 1s 169us/sample - loss: 0.0860 - accuracy: 0.9719\n",
      "Epoch 10/50\n",
      "7000/7000 [==============================] - 1s 135us/sample - loss: 0.0809 - accuracy: 0.9746\n",
      "Epoch 11/50\n",
      "7000/7000 [==============================] - 1s 151us/sample - loss: 0.0834 - accuracy: 0.9739\n",
      "Epoch 12/50\n",
      "7000/7000 [==============================] - 1s 175us/sample - loss: 0.0728 - accuracy: 0.9771\n",
      "Epoch 13/50\n",
      "7000/7000 [==============================] - 1s 173us/sample - loss: 0.0729 - accuracy: 0.9777\n",
      "Epoch 14/50\n",
      "7000/7000 [==============================] - 1s 170us/sample - loss: 0.0660 - accuracy: 0.9796\n",
      "Epoch 15/50\n",
      "7000/7000 [==============================] - 1s 188us/sample - loss: 0.0627 - accuracy: 0.9800\n",
      "Epoch 16/50\n",
      "7000/7000 [==============================] - 1s 167us/sample - loss: 0.0598 - accuracy: 0.9806\n",
      "Epoch 17/50\n",
      "7000/7000 [==============================] - 1s 212us/sample - loss: 0.0498 - accuracy: 0.9849\n",
      "Epoch 18/50\n",
      "7000/7000 [==============================] - 2s 236us/sample - loss: 0.0630 - accuracy: 0.9796\n",
      "Epoch 19/50\n",
      "7000/7000 [==============================] - 2s 226us/sample - loss: 0.0520 - accuracy: 0.9830\n",
      "Epoch 20/50\n",
      "7000/7000 [==============================] - 2s 234us/sample - loss: 0.0477 - accuracy: 0.9850\n",
      "Epoch 21/50\n",
      "7000/7000 [==============================] - 2s 232us/sample - loss: 0.0463 - accuracy: 0.9851\n",
      "Epoch 22/50\n",
      "7000/7000 [==============================] - 2s 260us/sample - loss: 0.0431 - accuracy: 0.9864\n",
      "Epoch 23/50\n",
      "7000/7000 [==============================] - 2s 232us/sample - loss: 0.0440 - accuracy: 0.9867\n",
      "Epoch 24/50\n",
      "7000/7000 [==============================] - 2s 224us/sample - loss: 0.0397 - accuracy: 0.9877\n",
      "Epoch 25/50\n",
      "7000/7000 [==============================] - 2s 281us/sample - loss: 0.0406 - accuracy: 0.9871\n",
      "Epoch 26/50\n",
      "7000/7000 [==============================] - 2s 261us/sample - loss: 0.0386 - accuracy: 0.9870\n",
      "Epoch 27/50\n",
      "7000/7000 [==============================] - 2s 226us/sample - loss: 0.0389 - accuracy: 0.9881\n",
      "Epoch 28/50\n",
      "7000/7000 [==============================] - 2s 252us/sample - loss: 0.0391 - accuracy: 0.9879\n",
      "Epoch 29/50\n",
      "7000/7000 [==============================] - 1s 205us/sample - loss: 0.0312 - accuracy: 0.9906\n",
      "Epoch 30/50\n",
      "7000/7000 [==============================] - 2s 273us/sample - loss: 0.0278 - accuracy: 0.9913\n",
      "Epoch 31/50\n",
      "7000/7000 [==============================] - 2s 229us/sample - loss: 0.0238 - accuracy: 0.9934\n",
      "Epoch 32/50\n",
      "7000/7000 [==============================] - 1s 187us/sample - loss: 0.0280 - accuracy: 0.9907\n",
      "Epoch 33/50\n",
      "7000/7000 [==============================] - 2s 226us/sample - loss: 0.0333 - accuracy: 0.9896\n",
      "Epoch 34/50\n",
      "7000/7000 [==============================] - 2s 223us/sample - loss: 0.0202 - accuracy: 0.9936\n",
      "Epoch 35/50\n",
      "7000/7000 [==============================] - 2s 224us/sample - loss: 0.0183 - accuracy: 0.9947\n",
      "Epoch 36/50\n",
      "7000/7000 [==============================] - 2s 223us/sample - loss: 0.0149 - accuracy: 0.9949\n",
      "Epoch 37/50\n",
      "7000/7000 [==============================] - 2s 221us/sample - loss: 0.0149 - accuracy: 0.9950\n",
      "Epoch 38/50\n",
      "7000/7000 [==============================] - 2s 227us/sample - loss: 0.0225 - accuracy: 0.9927\n",
      "Epoch 39/50\n",
      "7000/7000 [==============================] - 1s 211us/sample - loss: 0.0118 - accuracy: 0.9961\n",
      "Epoch 40/50\n",
      "7000/7000 [==============================] - 1s 103us/sample - loss: 0.0182 - accuracy: 0.9947\n",
      "Epoch 41/50\n",
      "7000/7000 [==============================] - 2s 215us/sample - loss: 0.0113 - accuracy: 0.9956\n",
      "Epoch 42/50\n",
      "7000/7000 [==============================] - 1s 181us/sample - loss: 0.0201 - accuracy: 0.9931\n",
      "Epoch 43/50\n",
      "7000/7000 [==============================] - 2s 223us/sample - loss: 0.0181 - accuracy: 0.9933\n",
      "Epoch 44/50\n",
      "7000/7000 [==============================] - 2s 274us/sample - loss: 0.0158 - accuracy: 0.9940\n",
      "Epoch 45/50\n",
      "7000/7000 [==============================] - 3s 382us/sample - loss: 0.0157 - accuracy: 0.9949\n",
      "Epoch 46/50\n",
      "7000/7000 [==============================] - 2s 253us/sample - loss: 0.0112 - accuracy: 0.9961\n",
      "Epoch 47/50\n",
      "7000/7000 [==============================] - 1s 125us/sample - loss: 0.0149 - accuracy: 0.9947\n",
      "Epoch 48/50\n",
      "7000/7000 [==============================] - 1s 126us/sample - loss: 0.0072 - accuracy: 0.9977\n",
      "Epoch 49/50\n",
      "7000/7000 [==============================] - 1s 127us/sample - loss: 0.0175 - accuracy: 0.9939\n",
      "Epoch 50/50\n",
      "7000/7000 [==============================] - 1s 127us/sample - loss: 0.0063 - accuracy: 0.9976\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f95ebe35710>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annelida_classifier.fit(X_train, y_train, \n",
    "                        batch_size = 100,\n",
    "                        epochs=50, \n",
    "                        verbose = 0) #switch to verbose = 1 if you want to watch it train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, we can use it to make predict classifications for our test data. Below we make these predictions, and then compare them to the known values to get an external measure of the annelid classifier's accuracy. Even with such a simplistic training approach we have produced a classifier that is >98% accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of model on classifying the test data:\n",
      "0.985\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#make preditions\n",
    "yht_dnn_vals = annelida_classifier.predict(X_test)\n",
    "\n",
    "#get the argmax of the predictions\n",
    "yht_dnn = np.argmax(yht_dnn_vals, axis = 1)\n",
    "\n",
    "#evaluate against the true values\n",
    "dnn_score = accuracy_score(y_test, yht_dnn)\n",
    "\n",
    "print(\"accuracy of model on classifying the test data:\")\n",
    "print(dnn_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the alfie `decode_predictions` function to move from binary classifications back to taxonomic names, just pass the list of names in alphabetical order (here these are lazily obtained using the original encoder's class list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Clitellata',\n",
       " 'Polychaeta',\n",
       " 'Polychaeta',\n",
       " 'Clitellata',\n",
       " 'Clitellata',\n",
       " 'Clitellata',\n",
       " 'Clitellata',\n",
       " 'Clitellata',\n",
       " 'Polychaeta',\n",
       " 'Polychaeta']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_class_predictions = decode_predictions(yht_dnn, tax_list = tax_encoder.classes_)\n",
    "\n",
    "y_test_class_predictions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying custom trained models\n",
    "\n",
    "Once you have optimized a custom model, you can save it to a file and then reuse it on novel data by employing alfie's `classify_records` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "##save and load via tensorflow\n",
    "\n",
    "##save model to file\n",
    "#annelida_classifier.save('annelida_demo_classifier')\n",
    "##load model from file\n",
    "#annelida_classifier = tf.keras.models.load_model('annelida_demo_classifier')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we move from the test dataframe to the list of dictionary format returned by alfie's `read_fasta` format (simulating reading new data in from a fasta file and evaluating it with our custom classifier). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'EWPC273-18',\n",
       " 'sequence': 'acactatattttatcctaggggtttgagccggcatggttggtgcgggcataagtctactcattcgaattgagctaagccaaccaggcgccttcttaggtagtgatcaactatataatacaattgttacagctcatgcatttgtaataattttcttcctagttatacctgtatttattggggggtttggaaactgacttctccccttaatactgggtgccccagacatagcatttccacgattaaacaatataaggttttggttactccctccttccctaatcctactggtgtcctcagctgcagtagaaaagggcgcaggtacaggttgaactgtataccccccactatcaagaaacctagcacatgcaggaccttctgtagacctagccattttttctcttcacttagccggggcatcttcaatcctgggggcaatcaactttattacaaccgttattaatatacgatgagcaggactacgtctagaacgaattcccctatttgtatgagctgtagtaatcacagtagttttacttcttctatccctcccagtgctagcaggagccattactatacttctcacagatcgaaacctaaatacctccttttttgacccagcgggagggggtgatcccattctatatcaacatcta'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new list of dictionaries\n",
    "test_simulated_fasta = []\n",
    "\n",
    "#only doing first 10 rows\n",
    "for i in range(0, 10):\n",
    "    x = test.iloc[i]\n",
    "    #make new dictionary entry for the sequence\n",
    "    new_record = {'name' : x['processid'], \n",
    "                 'sequence': x['sequence']}\n",
    "    #append to the list\n",
    "    test_simulated_fasta.append(new_record)\n",
    "\n",
    "#observe the data format\n",
    "test_simulated_fasta[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `classify_records` function can be used on data in this format, we just add the additional argument `dnn_model` to use the custom annelida classifying neural network as opposed to the default kingdom-level classifier. Our custom model uses a k-mer size of 4, so this parameter is not changed. For models trained on data for other k-mer sizes, the `k` argument must also be set to the correct value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_out, test_predictions = classify_records(test_simulated_fasta, dnn_model = annelida_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in part 1, this function yields the input sequences with the kmer_data added to their records, and a list of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'EWPC273-18', 'sequence': 'acactatattttatcctaggggtttgagccggcatggttggtgcgggcataagtctactcattcgaattgagctaagccaaccaggcgccttcttaggtagtgatcaactatataatacaattgttacagctcatgcatttgtaataattttcttcctagttatacctgtatttattggggggtttggaaactgacttctccccttaatactgggtgccccagacatagcatttccacgattaaacaatataaggttttggttactccctccttccctaatcctactggtgtcctcagctgcagtagaaaagggcgcaggtacaggttgaactgtataccccccactatcaagaaacctagcacatgcaggaccttctgtagacctagccattttttctcttcacttagccggggcatcttcaatcctgggggcaatcaactttattacaaccgttattaatatacgatgagcaggactacgtctagaacgaattcccctatttgtatgagctgtagtaatcacagtagttttacttcttctatccctcccagtgctagcaggagccattactatacttctcacagatcgaaacctaaatacctccttttttgacccagcgggagggggtgatcccattctatatcaacatcta', 'kmer_data': <alfie.kmerseq.KmerFeatures object at 0x7f95ebe69690>}\n",
      "predicted values:\n",
      "[0 1 1 1 0 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(test_out[0])\n",
    "\n",
    "print(\"predicted values:\")\n",
    "print(test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decode predictions function can then be used to move from numeric predictions back to class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Clitellata',\n",
       " 'Polychaeta',\n",
       " 'Polychaeta',\n",
       " 'Polychaeta',\n",
       " 'Clitellata',\n",
       " 'Clitellata',\n",
       " 'Clitellata',\n",
       " 'Polychaeta',\n",
       " 'Polychaeta',\n",
       " 'Polychaeta']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_predictions(test_predictions, tax_list = tax_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The custom models can be used from outside of python as well with the alfie command line function! Simply specify the custom model with the `-m` flag and the custom classes with the `-c` flag to conduct custom file-to-file classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few closing notes on the nuances and limitations of custom model training:\n",
    "\n",
    "- The alfie classification method is rapid, and accurate for higher taxonomic classes. It is however not a complete replacement for sequence alignment as it lacks the detail provided by sequence-to-sequence comparison. The initial purpose of alfie was to isolate sequences of interest from larger data sets and to narrow the search space for subsequent alignments. Use the package wisely!\n",
    "- The number of training samples and quality of data will influence how well a custom classifier performs, the performance of custom classifiers therefore cannot be guaranteed.\n",
    "- A common issue with multiclass taxonomic classification is class imbalance. For example you may have a phylum that contains 6 classes with the following sample counts:\n",
    "    ```\n",
    "    class_a = 2378\n",
    "    class_b = 1411\n",
    "    class_c = 1219\n",
    "    class_d = 377\n",
    "    class_e = 27\n",
    "    class_f = 12 \n",
    "    ```\n",
    "  With so few training instances for classes e and f, the neural network you build will likely classify individuals from these sparsely represented classes more poorly. In situations like this, you will have to get creative and try a few different things out. Possible solutions to this problem could be: upsampling the low frequency classes (using the `sample_seq` function), grouping the classes with smaller sample sizes into an 'other' category or adding them to the class of their closest taxonomic relatives, or possibly a series of binary classifier. No one of these solutions will work in all situations, so the best approach will depend on both your research goal and the data you're using.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
