{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The alfie package - demo\n",
    "\n",
    "\n",
    "## Part 1. Evaluate sequences with alfie's pre-built kingdom level classifier\n",
    "\n",
    "Alfie's primary function is as a kingdom-level taxonomic classifier for COI-5P barcode data. To accomplish this, alfie uses a deep neural network to analyze a set of input sequences and predict taxonomy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data \n",
    "\n",
    "Alfie contains a series of functions for reading and writing DNA sequence data in fasta or fastq format. These functions are found in the seqio module and their import is demonstrated below. Alfie also constains two example files that we import below using the read_fasta and read_fastq functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions for fasta/fastq input and output \n",
    "from alfie.seqio import read_fasta, read_fastq, write_fasta, write_fastq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': '@seq1_plantae',\n",
       " 'sequence': 'ttctaggagcatgtatatctatgctaatccgaatggaattagctcaaccaggtaaccatttgcttttaggtaatcaccaagtatacaatgttttaattacagcacatgcttttttaatgattttttttatggtaatgcctgtaatgattggtggttttggtaattggttagttcctattatgataggaagtccagatatggcttttcctagactaaataacatatctttttgacttcttccaccttctttatgtttacttttagcttcttcaatggttgaagtaggtgttggaacaggatgaactgtttatcctccccttagttcgatacaaagtcattcaggcggagctgttgatttagcaatttttagcttacatttatctggagcttcatcgattttaggagctgtcaattttatttctacgattctaaatatgcgtaatcctgggcaaagcatgtatcgaatgccattatttgtttgatctatttttgtaacggca',\n",
       " 'strand': '+',\n",
       " 'quality': '~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import path to example file\n",
    "from alfie import ex_fastq_file\n",
    "\n",
    "#read in example file\n",
    "example_fastq = read_fastq(ex_fastq_file)\n",
    "\n",
    "#check format\n",
    "example_fastq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'seq1_plantae',\n",
       " 'sequence': 'TTCTAGGAGCATGTATATCTATGCTAATCCGAATGGAATTAGCTCAACCAGGTAACCATTTGCTTTTAGGTAATCACCAAGTATACAATGTTTTAATTACAGCACATGCTTTTTTAATGATTTTTTTTATGGTAATGCCTGTAATGATTGGTGGTTTTGGTAATTGGTTAGTTCCTATTATGATAGGAAGTCCAGATATGGCTTTTCCTAGACTAAATAACATATCTTTTTGACTTCTTCCACCTTCTTTATGTTTACTTTTAGCTTCTTCAATGGTTGAAGTAGGTGTTGGAACAGGATGAACTGTTTATCCTCCCCTTAGTTCGATACAAAGTCATTCAGGCGGAGCTGTTGATTTAGCAATTTTTAGCTTACATTTATCTGGAGCTTCATCGATTTTAGGAGCTGTCAATTTTATTTCTACGATTCTAAATATGCGTAATCCTGGGCAAAGCATGTATCGAATGCCATTATTTGTTTGATCTATTTTTGTAACGGCA'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#repeat the above process with a fasta file\n",
    "from alfie import ex_fasta_file\n",
    "\n",
    "example_fasta = read_fasta(ex_fasta_file)\n",
    "\n",
    "example_fasta[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify sequences\n",
    "\n",
    "Once we have imported the data with the seqio module, we can use the `classify_records` function to obtain taxonomic classifications for the input sequences.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import classification function\n",
    "from alfie.classify import classify_records\n",
    "\n",
    "seq_records, predictions = classify_records(example_fasta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification process is completed above in just one line of code. In the background the `classify_records` function is taking the input sequences, generating a set of input features (k-mer frequencies) for each sequence, and passing the feature sets through a neural network to obtain a prediction. \n",
    "\n",
    "The function yields two outputs, a list of sequence records and an array classifications. The sequence records are a list of dictonaries, like the inputs, but now with an additional 'kmer_data' entry which contains a class instance used to generate the input features for the neural network (more information on the `KmerFeatures` class is provided in the following section on training a classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'seq1_plantae',\n",
       " 'sequence': 'TTCTAGGAGCATGTATATCTATGCTAATCCGAATGGAATTAGCTCAACCAGGTAACCATTTGCTTTTAGGTAATCACCAAGTATACAATGTTTTAATTACAGCACATGCTTTTTTAATGATTTTTTTTATGGTAATGCCTGTAATGATTGGTGGTTTTGGTAATTGGTTAGTTCCTATTATGATAGGAAGTCCAGATATGGCTTTTCCTAGACTAAATAACATATCTTTTTGACTTCTTCCACCTTCTTTATGTTTACTTTTAGCTTCTTCAATGGTTGAAGTAGGTGTTGGAACAGGATGAACTGTTTATCCTCCCCTTAGTTCGATACAAAGTCATTCAGGCGGAGCTGTTGATTTAGCAATTTTTAGCTTACATTTATCTGGAGCTTCATCGATTTTAGGAGCTGTCAATTTTATTTCTACGATTCTAAATATGCGTAATCCTGGGCAAAGCATGTATCGAATGCCATTATTTGTTTGATCTATTTTTGTAACGGCA',\n",
       " 'kmer_data': <alfie.kmerseq.KmerFeatures at 0x7f7b61b20950>}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_records[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions returned are an encoded array of kingdom classifications.\n",
    "Encodings are in alphabetical order: \n",
    "```\n",
    "0 == \"animalia\", 1 == \"bacteria\", 2 == \"fungi\", 3 == \"plantae\", 4 == \"protista\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1, 4, 0, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some purposes, the names corresponding to the encoded predictions may be preferable to the numeric encodings. To obtain these use the `decode_predictions` to move from the array of numeric predictions to a list of names.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plantae', 'bacteria', 'protista', 'animalia', 'animalia']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from alfie.classify import decode_predictions\n",
    "\n",
    "predicted_kingdoms = decode_predictions(predictions)\n",
    "predicted_kingdoms[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working within python provides the freedom to manipulate the sequence records in various ways using this information. What you do with the classification information will depend on your research goal, you may wish to save a select category of sequences to a file, merge the classifications with the existing sequence ids, or carry the classifications through to additional analyses in python. Here we explore a few of these possibilities.\n",
    "\n",
    "\n",
    "**a.** Add the predictions into the sequence dictonaries prior to subsequent manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'seq1_plantae',\n",
       " 'sequence': 'TTCTAGGAGCATGTATATCTATGCTAATCCGAATGGAATTAGCTCAACCAGGTAACCATTTGCTTTTAGGTAATCACCAAGTATACAATGTTTTAATTACAGCACATGCTTTTTTAATGATTTTTTTTATGGTAATGCCTGTAATGATTGGTGGTTTTGGTAATTGGTTAGTTCCTATTATGATAGGAAGTCCAGATATGGCTTTTCCTAGACTAAATAACATATCTTTTTGACTTCTTCCACCTTCTTTATGTTTACTTTTAGCTTCTTCAATGGTTGAAGTAGGTGTTGGAACAGGATGAACTGTTTATCCTCCCCTTAGTTCGATACAAAGTCATTCAGGCGGAGCTGTTGATTTAGCAATTTTTAGCTTACATTTATCTGGAGCTTCATCGATTTTAGGAGCTGTCAATTTTATTTCTACGATTCTAAATATGCGTAATCCTGGGCAAAGCATGTATCGAATGCCATTATTTGTTTGATCTATTTTTGTAACGGCA',\n",
       " 'kmer_data': <alfie.kmerseq.KmerFeatures at 0x7f7b61b20950>,\n",
       " 'kingdom': 'plantae'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#iterate over the predictions, use enumerate to get record number\n",
    "for i, p in enumerate(predicted_kingdoms):\n",
    "    #call corresponding sequence record and add a kingdom entry to dictonary\n",
    "    seq_records[i]['kingdom'] = p\n",
    "    \n",
    "#taxonomic classification now present in the sequence dict\n",
    "seq_records[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** Use the predictions to subset out only data from a kingdom of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_sequences = []\n",
    "\n",
    "for i, x in enumerate(predicted_kingdoms):\n",
    "    if x == 'animalia':\n",
    "        animal_sequences.append(seq_records[i])\n",
    "        \n",
    "#Note: you could avoid the transition to string classifications and \n",
    "#subset using the numberic classifications in the `predictions` array \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we see that the resulting list `animal_sequences` contains only the kingdom 'animalia'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'seq4_animalia',\n",
       "  'sequence': 'AATCCGGGATCATTAATTGGTGATGATCAAATTTATAATACCATTGTTACAGCTCATGCATTTATTATAATTTTTTTTATGGTTATACCAATTATAATCGGAGGATTTGGTAATTGATTAGTACCATTGATATTAGGGGCACCTGATATAGCTTTCCCACGAATAAATAATATAAGATTTTGATTACTACCCCCTTCTTTAATACTTCTAATTTCTAGTAGTATTGTAGAAAATGGAGCTGGAACTGGATGAACAGTTTACCCCCCTTTATCATCTAATATCGCCCATGGAGGAAGATCTGTTGACTTAGCTATTTTTTCATTACATTTAGCTGGTATTTCATCTATTTTAGGAGCTATTAATTTTATT',\n",
       "  'kmer_data': <alfie.kmerseq.KmerFeatures at 0x7f7b619ee050>,\n",
       "  'kingdom': 'animalia'},\n",
       " {'name': 'seq5_animalia',\n",
       "  'sequence': 'CAAATTTATAATACAATTGTTACAGCCCATGCTTTTATTATAATTTTCTTTATAGTAATGCCTATTATAATTGGAGGATTTGGAAATTGATTAGTACCTTTAATATTAGGAGCCCCCGATATAGCTTTCCCCCGAATAAATAATATAAGATTTTGACTTCTCCCCCCATCATTAACCCTTTTAATTTCAAGAAGAGTTGTAGAAAATGGTACTGGAACTGGATGAACAGTTTACCCCCCTTTATCATCTAATATTGCTCATAGAGGAAGATCTGTTGATTTATCTATTTTTTCCCTTCATTTAGCTGGAATTTCTTCTATTTTAGGAGCAATTAATTTTATTACAACTATTATTAATATACGATTAAATAATATAACATTTGATCAATTACCTTTATTTGTATGATCTGTTGGAATTACAGCTCTTCTTCTTCTTCTTTCTCTTCCTGTTTTAGCAGGAGCTATTACTATATTATTA',\n",
       "  'kmer_data': <alfie.kmerseq.KmerFeatures at 0x7f7b619f2190>,\n",
       "  'kingdom': 'animalia'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animal_sequences[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.** Writing to file\n",
    "\n",
    "Once processing and analyses are completed, we may wish to save our sequence records to a new output file. Sequences, or lists of sequences in dictonary format can be written to output files if they possess the proper set of keys ('name' and 'sequence' for fasta, 'name', 'sequence', 'strand', and 'quality'). Additional keys will be ignored when writing the output.\n",
    "\n",
    "In this demonstration, we take the animal sequences we isolated from the input (**b.**) and write them to a new fasta file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you uncomment and run the following line, it will make an output file in your current working directory\n",
    "#write_fasta( animal_sequences, 'animalia_example_output.fasta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats all there is to deploying the alfie package as a kingdom level classifier from within Python. The kingdom level classification provides an efficient means of separating DNA sequences from a target kingdom from the large amount of off-target noise that can exist within a metabarcoding or environmental DNA data set.\n",
    "\n",
    "Next, we explore how the functionality of alfie can be customized to allow for isolation of target sequences on finer taxonomic scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2. Train and test a custom, alignment-free taxonomic classifier\n",
    "\n",
    "In addition to using alfie as a kingdom level classifier, alfie's helper functions can be used to train a custom DNA barcode classification model. Custom model construction will allow for the general functionality of alfie (as a kingdom-level classifier) to be extended and specalized. Some common applications of this customization may be the training of a classifier for a sub-group of interest (i.e. an group classifier, which is demonstrated here for the phylum annelida), or training a binary classifier to isolate barcodes from a specific taxonomic group (i.e. a classifier that says whether an input sequence is or is not from a teleost fish).\n",
    "\n",
    "The following demonstration will show how to train a custom binary or multiclass neural network through a combination of alfie, scikit learn, and tensorflow. Note this process is a little more involved than the default implementation of alfie. This demo assumes an understanding of the basics of data science and machine learning in Python. If you're not yet comfortable with those topics, I would recommend the book [Hands-on Machine Learning with Scikit-Learn and TensorFlow](https://github.com/ageron/handson-ml2) as a good starting point.\n",
    "\n",
    "It is also important to note that your mileage with a design and deployment of custom classifier will vary with the quality of the training data you use. A few thousand labelled sequences at a minimum is recommended. If you're looking to acquire COI training data, consider: [the BOLD data systems website](http://www.boldsystems.org/index.php/Login/page?destination=MAS_Management_UserConsole), or [subsetting the data used in training the original alfie neural network](https://github.com/CNuge/data-alfie). \n",
    "\n",
    "If you want training data for other barcodes or genes have a look at [NCBI](https://www.ncbi.nlm.nih.gov) (warning: data mining and cleaning required), or other online barcode data sources such as the [PLANiTS dataset](https://github.com/apallavicini/PLANiTS). Another good source of barcode data is [Dr. Teresita Porter's GitHub page](https://github.com/terrimporter), which contains trained RDP Classifiers (and labelled training data!) for rbcL, 18S, ITS, and other barcodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alfie.kmerseq import KmerFeatures\n",
    "from alfie.training import stratified_taxon_split, sample_seq, process_sequences, alfie_dnn_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading the demo data\n",
    "\n",
    "The demo data can be found in the [alfie GitHub repository](https://github.com/CNuge/alfie/tree/master/alfie/data). The relative import below assumes you have downloaded the alfie repository from gihub and that your working directory is: `alfie/example`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../alfie/data/alfie_small_train_example.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processid</th>\n",
       "      <th>sequence</th>\n",
       "      <th>phylum</th>\n",
       "      <th>class</th>\n",
       "      <th>order</th>\n",
       "      <th>family</th>\n",
       "      <th>genus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>GAHAP309-13</td>\n",
       "      <td>accttatactttattctgggcgtatgagcaggaatattgggtgcag...</td>\n",
       "      <td>Annelida</td>\n",
       "      <td>Clitellata</td>\n",
       "      <td>Enchytraeida</td>\n",
       "      <td>Enchytraeidae</td>\n",
       "      <td>Grania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>GAHAP2002-14</td>\n",
       "      <td>accctatatttcattctcggagtttgagctggcatagtaggtgccg...</td>\n",
       "      <td>Annelida</td>\n",
       "      <td>Clitellata</td>\n",
       "      <td>Haplotaxida</td>\n",
       "      <td>Lumbricidae</td>\n",
       "      <td>Aporrectodea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>GBAN15302-19</td>\n",
       "      <td>actctatacttaatttttggtatt-gagccggtatagtaggaacag...</td>\n",
       "      <td>Annelida</td>\n",
       "      <td>Clitellata</td>\n",
       "      <td>Haplotaxida</td>\n",
       "      <td>Naididae</td>\n",
       "      <td>Ainudrilus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>GBAN11905-19</td>\n",
       "      <td>acactatattttattttaggaatttgagctggaataattggagcag...</td>\n",
       "      <td>Annelida</td>\n",
       "      <td>Clitellata</td>\n",
       "      <td>Crassiclitellata</td>\n",
       "      <td>Megascolecidae</td>\n",
       "      <td>Metaphire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>GBAN15299-19</td>\n",
       "      <td>acattatacctaattta-ggtgtatgagccggaatagttggaacag...</td>\n",
       "      <td>Annelida</td>\n",
       "      <td>Clitellata</td>\n",
       "      <td>Haplotaxida</td>\n",
       "      <td>Naididae</td>\n",
       "      <td>Ainudrilus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      processid                                           sequence    phylum  \\\n",
       "0   GAHAP309-13  accttatactttattctgggcgtatgagcaggaatattgggtgcag...  Annelida   \n",
       "1  GAHAP2002-14  accctatatttcattctcggagtttgagctggcatagtaggtgccg...  Annelida   \n",
       "2  GBAN15302-19  actctatacttaatttttggtatt-gagccggtatagtaggaacag...  Annelida   \n",
       "3  GBAN11905-19  acactatattttattttaggaatttgagctggaataattggagcag...  Annelida   \n",
       "4  GBAN15299-19  acattatacctaattta-ggtgtatgagccggaatagttggaacag...  Annelida   \n",
       "\n",
       "        class             order          family         genus  \n",
       "0  Clitellata      Enchytraeida   Enchytraeidae        Grania  \n",
       "1  Clitellata       Haplotaxida     Lumbricidae  Aporrectodea  \n",
       "2  Clitellata       Haplotaxida        Naididae    Ainudrilus  \n",
       "3  Clitellata  Crassiclitellata  Megascolecidae     Metaphire  \n",
       "4  Clitellata       Haplotaxida        Naididae    Ainudrilus  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For similicity, this demo is conducted with 10,000 sequences from the phylum Annelida, which has only two classes. We will train a model to predict the class for Annelida sequences. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Clitellata    6187\n",
       "Polychaeta    3813\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conducting a train/test split\n",
    "\n",
    "\n",
    "The aflie function `stratified_taxon_split` can be used to split a dataframe in a stratified fashion based on the taxnomic data in a column. This ensures that each taxonomic group is evenly represented in the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conducting train/test split, split evenly by: class\n"
     ]
    }
   ],
   "source": [
    "train, test = stratified_taxon_split(data, class_col = 'class', test_size = 0.3, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call some summary functions on the output dataframes to verify the even split of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 7)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (7000, 7)\n",
      "Clitellata    4331\n",
      "Polychaeta    2669\n",
      "Name: class, dtype: int64\n",
      "\n",
      "\n",
      "test data shape: (3000, 7)\n",
      "Clitellata    1856\n",
      "Polychaeta    1144\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape:\",train.shape)\n",
    "print(train['class'].value_counts())\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"test data shape:\", test.shape)\n",
    "print(test['class'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Clitellata    1856\n",
       "Polychaeta    1144\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the response data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding y arrays\n"
     ]
    }
   ],
   "source": [
    "print(\"encoding y arrays\")\n",
    "#encode the y labels\n",
    "y_train_raw =  train['class']\n",
    "y_test_raw = test['class']\n",
    "\n",
    "tax_encoder = LabelBinarizer()\n",
    "\n",
    "y_train = tax_encoder.fit_transform(y_train_raw)\n",
    "y_test = tax_encoder.transform(y_test_raw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 1)\n",
      "(3000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# uncomment these lines to save the arrays to files on your computer\n",
    "print(\"saving y to files\")\n",
    "np.save('example_y_train.npy', y_train)\n",
    "np.save('example_y_test.npy', y_test)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the predictor data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kmer features\n",
    "\n",
    "Alfie contains a custom python class called KmerFeatures, which intakes a DNA sequence and generates kmer count and kmer frequency data. This class is used in the background by the `classify_records` function to generate the features for neural network prediction. \n",
    "\n",
    "Here we will use it to take our data and generate the feature sets for model training. The class takes an id and a DNA sequence, by default it will count 4mer frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment to look at the docs\n",
    "#?KmerFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<alfie.kmerseq.KmerFeatures at 0x7f7b615142d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = KmerFeatures(name = train['processid'][0]  , sequence = train['sequence'][0])\n",
    "x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon initiation, the KmerFeatures class instance will generate a kmer count dictonary, where the keys are all the nucleotide permutations for size 'k'. The class then iterates through the input sequence and counts the occurances of each kmer. Any occurances of nucleotides other than A, T, G, and C will cause the encompassing kmer to be ignored.\n",
    "\n",
    "After initiation, the kmer keys and values can be accessed like a regular python dictonary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AAAA',\n",
       " 'AAAC',\n",
       " 'AAAG',\n",
       " 'AAAT',\n",
       " 'AACA',\n",
       " 'AACC',\n",
       " 'AACG',\n",
       " 'AACT',\n",
       " 'AAGA',\n",
       " 'AAGC']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.keys()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Note/digression: later on, we rely on the fact keys are from an alphabetically ordered dict (the default for python 3.6 or newer). Alfie sorts the dict items by key before returning them just to be safe (Python 3.6 and later are ordered dicts by dfault so this is redundant). This is a PSA that it is 2020 and time to update if you're on 3.5 or earlier!](https://twitter.com/raymondh/status/773978885092323328)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 1, 2, 2, 3, 1, 4, 3, 1]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.values()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AAAA', 2),\n",
       " ('AAAC', 3),\n",
       " ('AAAG', 1),\n",
       " ('AAAT', 2),\n",
       " ('AACA', 2),\n",
       " ('AACC', 3),\n",
       " ('AACG', 1),\n",
       " ('AACT', 4),\n",
       " ('AAGA', 3),\n",
       " ('AAGC', 1)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.items()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that the training data is not biased by the overall size of the sequences, it is best to train the models using the kmer frequencies (count/total), which the KmerFeatures class also provides for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0030722 , 0.00460829, 0.0015361 , 0.0030722 , 0.0030722 ,\n",
       "       0.00460829, 0.0015361 , 0.00614439, 0.00460829, 0.0015361 ])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.freq_values()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For machine learning purposes, the KmerFeatures class also outputs the kmer dictonary keys and value frequencies as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AAAA', 'AAAC', 'AAAG', 'AAAT', 'AACA'], dtype='<U4')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.labels[:5] #only showing first 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0030722 , 0.00460829, 0.0015361 , 0.0030722 , 0.0030722 ])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.kmer_freqs[:5] #just static method version of x1.freq_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By passing the optional kmers argument, we can overrule the default kmer size of 4 and specify a custom kmer dictonary size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AAAAA', 1), ('AAAAC', 0), ('AAAAG', 1), ('AAAAT', 0), ('AAACA', 0)]\n",
      "[('A', 184), ('C', 134), ('G', 110), ('T', 226)]\n"
     ]
    }
   ],
   "source": [
    "# count 5mers \n",
    "x5mer = KmerFeatures(name = train['processid'][0]  , sequence = train['sequence'][0], kmers = 5)\n",
    "\n",
    "print(x5mer.items()[:5])\n",
    "\n",
    "#generate 5mer and 1mer frequencies\n",
    "x1_mer = KmerFeatures(name = train['processid'][0]  , sequence = train['sequence'][0], kmers = 1)\n",
    "print(x1_mer.items())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### random subsampling\n",
    "\n",
    "The kmer encoding of the predictor data above utilizes whole barcode records. Often in analysis of metabarcode or eDNA research we are not dealing with complete barcode sequences, but rather short sequences (from primer-defined barcode sub-sections in the case of metabarcoding data, or undefined sub-sections in the case of metagenomics data). \n",
    "\n",
    "To train our classification model on data that more closely resembles real world sequences, we can use the alfie's `sample_seq` function to randomly subsample the training sequences. We specify the `min_size` and `max_size` of the subsequence and the function will randomly generate a subsample of the sequence in the defined size range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?sample_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['attgaattaagccaaccaggatcatttttaggaagagaccaactatataatacaattgtcacagcacatgcatttttaataattttttttctagttataccagtatttattggcggatttggaaactgacttctcccattaatacttggagcccctgacatagcattcccacgattaaataatataagattttgattactacctccgtctctcattcttcttgtttcatccgcagctgttgaaaaaggagcaggaacaggttgaactgtatatccacctctagcaaggaatttagcacatgcaggtccttc']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#demonstrate on a single sequence - i.e. row 1 of the train\n",
    "\n",
    "sub_seq = sample_seq(train['sequence'][0])\n",
    "sub_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the sample seq function upsampling of data as well. If we an integer value for the  `n` argument to the function then `n` random subsamples will be generated. The function therefore returns a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cttatactttattctgggcgtatgagcaggaatattgggtgcagcgataagattgttaattcgaattgaattaagccaaccaggatcatttttaggaagagaccaactatataatacaattgtcacagcacatgcatttttaataattttttttctagttataccagtatttattggcggatttggaaactgacttctcccattaatacttggagcccctgacatagcattcccacgattaaataatataagattttgattactacctccgtctctcattcttcttgtttcatccgcagctgttgaaaaaggagcaggaacaggttgaactgtatatccacctctagcaaggaatttagcacatgcaggtccttcagtagatttagcaatcttttcacttcatctcgcaggtgcttcctctattttaggtgcagtaaactttattactacagtaattaatatgcgttgacaaggaatctctctagaacgaatccccttatttgtatgagctgtagctattacagttgttctattacttttatctcttccagttcttgctggagccattactatatt',\n",
       " 'gaattgaattaagccaaccaggatcatttttaggaagagaccaactatataatacaattgtcacagcacatgcatttttaataattttttttctagttataccagtatttattggcggatttggaaactgacttctcccattaatacttggagcccctgacatagcattcccacgattaaataatataagattttgattacta']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_seq = sample_seq(train['sequence'][0], n = 10)\n",
    "sub_seq[:2] #different random subsections of the same input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### batch processing\n",
    "We can batch process sequences, taking random subsamples, generating `KmerFeatures` class instances, and extracting the kmer frequencies from the objects using the `process_sequences` function. This function takes in a dataframe contains the ids, sequences and labels in different columns. The function will generate a dictonary that contains four lists of: ids, sequences, kmer frequency arrays, and labels for each sequence.\n",
    "\n",
    "Here both the train and test data are processed with the defaults kmer size (`k = 4`) and a single subsample per sequence in the input dataframe (`n = 1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?process_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kmer_data = process_sequences(train, label_col = 'class')\n",
    "test_kmer_data = process_sequences(test, label_col = 'class')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ids', 'labels', 'data', 'seq'])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_kmer_data.keys() #four keys, each with lists of equal size containing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs dictonaries of the process_sequences function can be easily turned into numpy arrays, which are compatible inputs for most machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building X arrays\n"
     ]
    }
   ],
   "source": [
    "print(\"building X arrays\")\n",
    "X_train = np.array(train_kmer_data['data'])\n",
    "X_test = np.array(test_kmer_data['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit learn's label binarizer function can be used to turn the labels into numeric encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding y arrays\n",
      "names are encoded in alphabetical order:\n",
      "['Clitellata' 'Polychaeta']\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "(7000, 1)\n",
      "(3000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"encoding y arrays\")\n",
    "#encode the y labels\n",
    "y_train_raw =  train_kmer_data['labels']\n",
    "y_test_raw = test_kmer_data['labels']\n",
    "\n",
    "tax_encoder = LabelBinarizer()\n",
    "\n",
    "y_train = tax_encoder.fit_transform(y_train_raw)\n",
    "y_test = tax_encoder.transform(y_test_raw)\n",
    "\n",
    "print(\"names are encoded in alphabetical order:\")\n",
    "print(tax_encoder.classes_)\n",
    "\n",
    "# note since this is a binary class the y arrays contain only 0 and 1\n",
    "# if multiclass, we need to take the argmax for multilabels \n",
    "#y_train = np.argmax(y_train, axis = 1)\n",
    "#y_test = np.argmax(y_train, axis = 1)\n",
    "\n",
    "print(y_train[:10])\n",
    "print(y_train[-10:])\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uncomment these lines to save the arrays to your computer\n",
    "#print(\"saving X to files\")\n",
    "#np.save('X_train.npy', X_train)\n",
    "#np.save('X_test.npy', X_test)\n",
    "\n",
    "#print(\"saving y to files\")\n",
    "#np.save('y_train.npy', y_train)\n",
    "#np.save('y_test.npy', y_test)\n",
    "\n",
    "##can then load with np.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?alfie_dnn_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "annelida_params = {\n",
    "    'hidden_sizes' : [16,64,128,64,16], # the number of neurons in our hidden layers \n",
    "    'dropout' : 0.2, #the dropout rate for the neural networl\n",
    "    'in_shape' : 256, #default for 4mers, can determine this through the shape of the X dataframe (# columns)  \n",
    "    'n_classes' : 2, #we have two annelida classes. Default is 5 (number of kingdoms).    \n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "annelida_classifier = alfie_dnn_default(**annelida_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 100)               25700     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                1616      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                1088      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 46,054\n",
      "Trainable params: 46,054\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#look at the neural network architecture\n",
    "annelida_classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the classifier is trained in a simple fashion, the data is passed through the model in batches of 100 for 50 epochs. We could improve this model with cross validation and hyperparamater tuning (i.e. changing the dropout or the number of hidden layers). Additionally you could change the set of input features (size of k), or take lower level control and design a neural network youself. These refinements and optimizations are outside of the scope of the alfie tutorial. Scikit learn and tensorflow have excellent functions and documentation if you wish to branch out beyond the `alfie_dnn_default` architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples\n",
      "Epoch 1/30\n",
      "7000/7000 [==============================] - 2s 244us/sample - loss: 0.5907 - accuracy: 0.6387\n",
      "Epoch 2/30\n",
      "7000/7000 [==============================] - 1s 132us/sample - loss: 0.2932 - accuracy: 0.9183\n",
      "Epoch 3/30\n",
      "7000/7000 [==============================] - 1s 140us/sample - loss: 0.1522 - accuracy: 0.9503\n",
      "Epoch 4/30\n",
      "7000/7000 [==============================] - 1s 137us/sample - loss: 0.1201 - accuracy: 0.9613\n",
      "Epoch 5/30\n",
      "7000/7000 [==============================] - 1s 148us/sample - loss: 0.1043 - accuracy: 0.9669\n",
      "Epoch 6/30\n",
      "7000/7000 [==============================] - 1s 172us/sample - loss: 0.0916 - accuracy: 0.9701\n",
      "Epoch 7/30\n",
      "7000/7000 [==============================] - 1s 144us/sample - loss: 0.0921 - accuracy: 0.9707\n",
      "Epoch 8/30\n",
      "7000/7000 [==============================] - 1s 129us/sample - loss: 0.0790 - accuracy: 0.9764\n",
      "Epoch 9/30\n",
      "7000/7000 [==============================] - 1s 157us/sample - loss: 0.0717 - accuracy: 0.9777\n",
      "Epoch 10/30\n",
      "7000/7000 [==============================] - 1s 158us/sample - loss: 0.0789 - accuracy: 0.9757\n",
      "Epoch 11/30\n",
      "7000/7000 [==============================] - 1s 175us/sample - loss: 0.0709 - accuracy: 0.9790\n",
      "Epoch 12/30\n",
      "7000/7000 [==============================] - 1s 176us/sample - loss: 0.0718 - accuracy: 0.9767\n",
      "Epoch 13/30\n",
      "7000/7000 [==============================] - 1s 174us/sample - loss: 0.0622 - accuracy: 0.9823\n",
      "Epoch 14/30\n",
      "7000/7000 [==============================] - 1s 160us/sample - loss: 0.0606 - accuracy: 0.9804\n",
      "Epoch 15/30\n",
      "7000/7000 [==============================] - 1s 187us/sample - loss: 0.0559 - accuracy: 0.9841\n",
      "Epoch 16/30\n",
      "7000/7000 [==============================] - 1s 189us/sample - loss: 0.0626 - accuracy: 0.9800\n",
      "Epoch 17/30\n",
      "7000/7000 [==============================] - 1s 191us/sample - loss: 0.0539 - accuracy: 0.9846\n",
      "Epoch 18/30\n",
      "7000/7000 [==============================] - 2s 265us/sample - loss: 0.0572 - accuracy: 0.9806\n",
      "Epoch 19/30\n",
      "7000/7000 [==============================] - 1s 190us/sample - loss: 0.0465 - accuracy: 0.9853\n",
      "Epoch 20/30\n",
      "7000/7000 [==============================] - 1s 182us/sample - loss: 0.0424 - accuracy: 0.9879\n",
      "Epoch 21/30\n",
      "7000/7000 [==============================] - 1s 190us/sample - loss: 0.0351 - accuracy: 0.9890\n",
      "Epoch 22/30\n",
      "7000/7000 [==============================] - 1s 127us/sample - loss: 0.0374 - accuracy: 0.9886\n",
      "Epoch 23/30\n",
      "7000/7000 [==============================] - 1s 201us/sample - loss: 0.0607 - accuracy: 0.9797\n",
      "Epoch 24/30\n",
      "7000/7000 [==============================] - 1s 166us/sample - loss: 0.0346 - accuracy: 0.9893\n",
      "Epoch 25/30\n",
      "7000/7000 [==============================] - 2s 282us/sample - loss: 0.0324 - accuracy: 0.9900\n",
      "Epoch 26/30\n",
      "7000/7000 [==============================] - 1s 214us/sample - loss: 0.0321 - accuracy: 0.9901\n",
      "Epoch 27/30\n",
      "7000/7000 [==============================] - 2s 254us/sample - loss: 0.0251 - accuracy: 0.9930\n",
      "Epoch 28/30\n",
      "7000/7000 [==============================] - 2s 271us/sample - loss: 0.0345 - accuracy: 0.9886\n",
      "Epoch 29/30\n",
      "7000/7000 [==============================] - 2s 266us/sample - loss: 0.0215 - accuracy: 0.9940\n",
      "Epoch 30/30\n",
      "7000/7000 [==============================] - 2s 257us/sample - loss: 0.0247 - accuracy: 0.9933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7ab4dcbc90>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annelida_classifier.fit(X_train, y_train, \n",
    "                        batch_size = 100,\n",
    "                        epochs=50, \n",
    "                        verbose = 1) #switch to verbose = 0 if you want to watch it train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, we can use it to make predict classifications for our test data. Below we make these predictions, and then compare them to the know values to get an external measure of the annelid classifier's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of model on classifying the test data:\n",
      "0.9826666666666667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#make preditions\n",
    "yht_dnn_vals = annelida_classifier.predict(X_test)\n",
    "\n",
    "#get the argmax of the predictions\n",
    "yht_dnn = np.argmax(yht_dnn_vals, axis = 1)\n",
    "\n",
    "#evaluate against the true values\n",
    "dnn_score = accuracy_score(y_test, yht_dnn)\n",
    "\n",
    "print(\"accuracy of model on classifying the test data:\")\n",
    "print(dnn_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the alfie `decode_predictions` function to move from binary classifications back to taxonomic names, just pass then list of names in alphabetical order (here this are lazily obtained using the original encoder's class list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Polychaeta',\n",
       " 'Clitellata',\n",
       " 'Clitellata',\n",
       " 'Polychaeta',\n",
       " 'Clitellata',\n",
       " 'Polychaeta',\n",
       " 'Clitellata',\n",
       " 'Clitellata',\n",
       " 'Clitellata',\n",
       " 'Clitellata']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_class_predictions = decode_predictions(yht_dnn, tax_list = tax_encoder.classes_)\n",
    "\n",
    "y_test_class_predictions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploying custom trained models\n",
    "\n",
    "Once you have optimized a custom model, you can save it to a file and then reuse on novel data while taking advantage of the alfie `classify_records` function for kmer-ization of inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##save and load via tensorflow\n",
    "\n",
    "##save model to file\n",
    "#annelida_classifier.save('annelida_demo_classifier')\n",
    "##load model from file\n",
    "#annelida_classifier = tf.keras.models.load_model('annelida_demo_classifier')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we move from the test dataframe to the list of dictonary format returned by alfie's `read_fasta` format (simulating reading new data in from a fasta file and evaluating it with our custom classifier). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'KBPOL721-11',\n",
       " 'sequence': 'acactatattttatttttggaacctgatctggcttattgggcacctccataaggatgcttattcgtgctgaattaggacaacccgggtctttactaggaagagatcagctttataatacaattgtgacagcacatgcgtttctaataatttttttcttagtcatgcctatcttagtaggagggttcggtaattgacttatcccccttatattaggggctcctgatatagcattcccccgtctaaacaacataagattctggttattgccgccctcactaattcttttattaagatcaagtgcggttgaaaagggggttggaactggatgaacagtctacccccctctagcagcaaacattgcccacgctggaccttcagttgacctagctattttttcgcttcatattgcaggagtttcatcaattctaggggcattaaatttcatcaccacagtccttaatatacggtataaaggactacgattagaacgggtacctttatttgtttgagctgctaaagtaaccgccattctattacttctaaggctccctgtattagctggtgcaattaccatactactaacagaccgtaatttaaacactgctttctttgaccctgcgggtggaggagacccaattctctaccaacattta'}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new list of dictonaries\n",
    "test_simulated_fasta = []\n",
    "\n",
    "#only doing first 10 rows\n",
    "for i in range(0, 10):\n",
    "    x = test.iloc[i]\n",
    "    #make new dictonary entry for the sequence\n",
    "    new_record = {'name' : x['processid'], \n",
    "                 'sequence': x['sequence']}\n",
    "    #append to the list\n",
    "    test_simulated_fasta.append(new_record)\n",
    "\n",
    "#observe the data format\n",
    "test_simulated_fasta[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `classify_records` function can be used, we just add the additional argument `dnn_model` to deploy our custom annelida classifying neural network as opposed to the default kingdom-level classifier. Our custom model uses a kmer size of 4, so this parameter is not changed. For models trained on data for other kmer sizes, the `k` argument must also be set to the correct value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_out, test_predictions = classify_records(test_simulated_fasta, dnn_model = annelida_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in part 1, this function yields the set of input sequences with the kmer_data addded, and a list of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'KBPOL721-11', 'sequence': 'acactatattttatttttggaacctgatctggcttattgggcacctccataaggatgcttattcgtgctgaattaggacaacccgggtctttactaggaagagatcagctttataatacaattgtgacagcacatgcgtttctaataatttttttcttagtcatgcctatcttagtaggagggttcggtaattgacttatcccccttatattaggggctcctgatatagcattcccccgtctaaacaacataagattctggttattgccgccctcactaattcttttattaagatcaagtgcggttgaaaagggggttggaactggatgaacagtctacccccctctagcagcaaacattgcccacgctggaccttcagttgacctagctattttttcgcttcatattgcaggagtttcatcaattctaggggcattaaatttcatcaccacagtccttaatatacggtataaaggactacgattagaacgggtacctttatttgtttgagctgctaaagtaaccgccattctattacttctaaggctccctgtattagctggtgcaattaccatactactaacagaccgtaatttaaacactgctttctttgaccctgcgggtggaggagacccaattctctaccaacattta', 'kmer_data': <alfie.kmerseq.KmerFeatures object at 0x7f7ae58014d0>}\n",
      "predicted values\n",
      "[1 0 0 1 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(test_out[0])\n",
    "\n",
    "print(\"predicted values:\")\n",
    "print(test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously shown, the decode predictions function can be used to move from numeric predictions back to class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Polychaeta',\n",
       " 'Clitellata',\n",
       " 'Clitellata',\n",
       " 'Polychaeta',\n",
       " 'Clitellata',\n",
       " 'Polychaeta',\n",
       " 'Clitellata',\n",
       " 'Clitellata',\n",
       " 'Clitellata',\n",
       " 'Clitellata']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_predictions(test_predictions, tax_list = tax_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The custom models can be used from outside of python as well using the alfie command line function! Simply specify the custom model with the `-m` flag and the custom classes with the `-c` flag to conduct custom file-to-file classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few closing notes on the nuances and limitations of custom model training:\n",
    "\n",
    "- The classification provided here is rapid and accurate for higher taxonomic classes tested thus far. It is however not a complete replacement for sequence alignment as it lacks the detail provided by sequence to sequence comparison. The initial purpose of alfie was to isolate sequences of interest from larger data sets and to narrow the search space for subsequent alignments. Use the package wisely!\n",
    "- The number of training samples and quality of data will influence how well a custom classifer performs, the performance of custom classifiers therefore cannot be guaranteed.\n",
    "- A common issue with multiclass taxonomic classification is class imbalance. For example you may have a phylum that contains 6 classes with the following sample counts:\n",
    "    - class_a = 2378\n",
    "    - class_b = 1411\n",
    "    - class_c = 1219\n",
    "    - class_d = 377\n",
    "    - class_e = 27\n",
    "    - class_f = 12 \n",
    "    \n",
    "   With so few training instances for classes e and f, the neural network you build will likely classify individuals from these sparsely represented classes more poorly. In situations like this, you will have to get creative and try a few different things out. Possible solutions to this problem could be: upsampling the low frequency classes (using the `sample_seq` function), grouping the classes with smaller sample sizes into an 'other' category or grouping them into the class of their closest taxonomic relatives, or possibly a series of binary classifier. No one of these solutions will work in all situations, so the best approach will depend on both your research goal and the data you're using.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
